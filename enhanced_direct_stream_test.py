#!/usr/bin/env python3
"""
Enhanced Direct Stream Test with Complete Output System
======================================================

Demonstrates the complete enhanced output system integration with the existing
direct_stream_test.py, showing how to generate multi-format outputs, hierarchical
navigation, actionable insights, monitoring integration, and real-time dashboards.

This serves as a practical example of how teams would actually use the enhanced
output system in production environments.

Usage:
    python enhanced_direct_stream_test.py 4           # 4 streams with enhanced outputs
    python enhanced_direct_stream_test.py 4 300       # 4 streams, 300s duration
    python enhanced_direct_stream_test.py 4 300 --dashboard  # Start with dashboard
"""

import asyncio
import sys
import os
import time
import logging
import json
from datetime import datetime
from pathlib import Path

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import existing components
from direct_stream_test import DirectLoadTester

# Import enhanced output system components
from enhanced_output_system import EnhancedOutputSystem, ReportConfig, AudienceConfig
from hierarchical_navigation_system import HierarchicalNavigationSystem, FilterCriteria
from actionable_insights_system import ActionableInsightsSystem, Priority, ImpactLevel, Urgency
from integration_system import IntegrationSystem, IntegrationConfig, NotificationConfig, MonitoringIntegration
from dashboard_system import RealTimeDashboard, MetricsStorage, BaselineTracker, MetricPoint, AlertThreshold

class EnhancedDirectLoadTester:
    """Enhanced version of DirectLoadTester with complete output system integration"""
    
    def __init__(self, stream_count: int, test_duration: int = 120, enable_dashboard: bool = False):
        # Initialize base tester
        self.base_tester = DirectLoadTester(stream_count, test_duration)
        self.stream_count = stream_count
        self.test_duration = test_duration
        self.enable_dashboard = enable_dashboard
        
        # Initialize enhanced output components
        self.output_system = EnhancedOutputSystem("enhanced_reports")
        self.navigation_system = HierarchicalNavigationSystem("organized_reports") 
        self.insights_system = ActionableInsightsSystem()
        self.integration_system = IntegrationSystem()
        
        # Initialize dashboard components if enabled
        if enable_dashboard:
            self.metrics_storage = MetricsStorage("load_test_metrics.db")
            self.baseline_tracker = BaselineTracker(self.metrics_storage)
            self.dashboard = RealTimeDashboard(self.metrics_storage, self.baseline_tracker)
            self._setup_dashboard()
        
        # Setup logging
        self.logger = logging.getLogger(f"EnhancedDirectLoadTester_{id(self)}")
        self.logger.setLevel(logging.INFO)
        
        # Create console handler if not exists
        if not self.logger.handlers:
            console_handler = logging.StreamHandler()
            console_handler.setLevel(logging.INFO)
            formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
            console_handler.setFormatter(formatter)
            self.logger.addHandler(console_handler)
    
    def _setup_dashboard(self):
        """Setup dashboard with alert thresholds and monitoring"""
        # Setup alert thresholds
        self.dashboard.add_alert_threshold(AlertThreshold(
            metric_name="avg_fps",
            operator="lt",
            value=15.0,
            severity="warning"
        ))
        
        self.dashboard.add_alert_threshold(AlertThreshold(
            metric_name="avg_fps", 
            operator="lt",
            value=10.0,
            severity="critical"
        ))
        
        self.dashboard.add_alert_threshold(AlertThreshold(
            metric_name="cpu_usage",
            operator="gt",
            value=80.0,
            severity="warning"
        ))
        
        self.dashboard.add_alert_threshold(AlertThreshold(
            metric_name="cpu_usage",
            operator="gt", 
            value=90.0,
            severity="critical"
        ))
    
    async def run_enhanced_test(self) -> dict:
        """Run the enhanced load test with complete output generation"""
        
        self.logger.info(f"\n{'='*80}")
        self.logger.info(f"ğŸš€ ENHANCED DIRECT LOAD TEST")
        self.logger.info(f"Testing {self.stream_count} concurrent streams with enhanced outputs")
        self.logger.info(f"Duration: {self.test_duration}s | Dashboard: {'Enabled' if self.enable_dashboard else 'Disabled'}")
        self.logger.info(f"{'='*80}")
        
        # Start dashboard in background if enabled
        dashboard_task = None
        if self.enable_dashboard:
            dashboard_task = asyncio.create_task(self._start_dashboard_background())
            await asyncio.sleep(2)  # Give dashboard time to start
            self.logger.info("ğŸ“Š Dashboard started at http://localhost:5000")
        
        try:
            # Run the base load test
            self.logger.info("ğŸ§ª Running base load test...")
            start_time = time.time()
            
            # If dashboard is enabled, start real-time metrics collection
            metrics_task = None
            if self.enable_dashboard:
                metrics_task = asyncio.create_task(self._collect_realtime_metrics())
            
            # Run the actual test
            base_report = await self.base_tester.run_direct_test()
            
            if "error" in base_report:
                self.logger.error(f"âŒ Base test failed: {base_report['error']}")
                return {"error": base_report["error"]}
            
            # Stop metrics collection
            if metrics_task:
                metrics_task.cancel()
                try:
                    await metrics_task
                except asyncio.CancelledError:
                    pass
            
            test_duration = time.time() - start_time
            
            # Generate enhanced outputs
            self.logger.info("ğŸ“ˆ Generating enhanced analysis and outputs...")
            enhanced_outputs = await self._generate_enhanced_outputs(base_report, test_duration)
            
            # Display comprehensive results
            self._display_enhanced_results(enhanced_outputs)
            
            return enhanced_outputs
            
        except Exception as e:
            self.logger.error(f"âŒ Enhanced test failed: {e}")
            return {"error": str(e)}
        
        finally:
            # Clean up dashboard if it was started
            if dashboard_task and not dashboard_task.done():
                dashboard_task.cancel()
                try:
                    await dashboard_task
                except asyncio.CancelledError:
                    pass
    
    async def _start_dashboard_background(self):
        """Start dashboard server in background"""
        try:
            # This would normally start the Flask server
            # For demo purposes, we'll simulate dashboard startup
            self.logger.info("Dashboard server started on http://localhost:5000")
            
            # Keep the dashboard running
            while True:
                await asyncio.sleep(1)
                
        except asyncio.CancelledError:
            self.logger.info("Dashboard server stopped")
    
    async def _collect_realtime_metrics(self):
        """Collect real-time metrics during test execution"""
        try:
            while True:
                # Simulate real-time metrics collection
                # In real implementation, this would collect from the actual test
                
                # Add some sample metrics
                current_time = datetime.now()
                
                # CPU usage (simulated)
                cpu_usage = 45.0 + (time.time() % 10) * 3  # Varies between 45-75%
                self.dashboard.add_metric("cpu_usage", cpu_usage, {"source": "system"})
                
                # Memory usage (simulated)
                memory_usage = 30.0 + (time.time() % 8) * 2  # Varies between 30-46%
                self.dashboard.add_metric("memory_usage", memory_usage, {"source": "system"})
                
                # FPS (simulated)
                fps = 18.0 + (time.time() % 5)  # Varies between 18-23
                self.dashboard.add_metric("avg_fps", fps, {"source": "streams"})
                
                # Active streams
                self.dashboard.add_metric("active_streams", self.stream_count, {"source": "test"})
                
                await asyncio.sleep(5)  # Update every 5 seconds
                
        except asyncio.CancelledError:
            self.logger.info("Real-time metrics collection stopped")
    
    async def _generate_enhanced_outputs(self, base_report: dict, test_duration: float) -> dict:
        """Generate all enhanced outputs and analysis"""
        
        enhanced_outputs = {
            "timestamp": datetime.now().isoformat(),
            "test_info": {
                "stream_count": self.stream_count,
                "test_duration": self.test_duration,
                "actual_duration": test_duration,
                "enhanced_features_enabled": True
            },
            "base_report": base_report
        }
        
        # Step 1: Generate enhanced data structure
        self.logger.info("  ğŸ“Š Enhancing data structure...")
        test_config = {
            "stream_count": self.stream_count,
            "test_duration": self.test_duration,
            "enhanced_mode": True
        }
        
        # Define audience configurations
        audiences = [
            AudienceConfig("data_scientist", "high", "deep", True, True),
            AudienceConfig("engineer", "high", "moderate", True, True), 
            AudienceConfig("executive", "low", "summary", True, False),
            AudienceConfig("operations", "medium", "moderate", True, False)
        ]
        
        # Step 2: Generate multi-format outputs
        self.logger.info("  ğŸ“„ Generating multi-format reports...")
        format_results = self.output_system.generate_all_formats(
            base_report, test_config, audiences
        )
        enhanced_outputs["format_results"] = format_results
        
        # Step 3: Create hierarchical navigation
        self.logger.info("  ğŸ—‚ï¸  Creating hierarchical navigation...")
        enhanced_data = self.output_system._enhance_report_data(base_report, test_config)
        organized_data = self.navigation_system.organize_report_data(enhanced_data)
        enhanced_outputs["organized_data"] = organized_data
        
        # Generate navigation UI for different audiences
        navigation_files = {}
        for audience in ["executive", "engineer", "data_scientist", "operations"]:
            nav_file = self.navigation_system.save_navigation_ui(organized_data, audience)
            navigation_files[audience] = nav_file
        enhanced_outputs["navigation_files"] = navigation_files
        
        # Step 4: Generate actionable insights
        self.logger.info("  ğŸ’¡ Generating actionable insights...")
        actionable_insights = self.insights_system.generate_actionable_insights(enhanced_data)
        enhanced_outputs["actionable_insights"] = actionable_insights
        
        # Generate recommendations in different formats
        recommendations = actionable_insights.get("recommendations", [])
        enhanced_outputs["recommendations_json"] = self.insights_system.export_recommendations(recommendations, "json")
        enhanced_outputs["recommendations_csv"] = self.insights_system.export_recommendations(recommendations, "csv")
        
        # Generate ticket integration data
        ticket_data = self.insights_system.generate_ticket_integration_data(recommendations)
        enhanced_outputs["ticket_integration_data"] = ticket_data
        
        # Step 5: Setup integrations (simulated)
        self.logger.info("  ğŸ”— Setting up integrations...")
        integration_results = await self.integration_system.integrate_test_results(enhanced_data, test_config)
        enhanced_outputs["integration_results"] = integration_results
        
        # Step 6: Dashboard and metrics analysis
        if self.enable_dashboard:
            self.logger.info("  ğŸ“Š Generating dashboard analysis...")
            # Generate performance report from collected metrics
            start_time = datetime.now() - timedelta(seconds=test_duration + 60)
            end_time = datetime.now()
            
            performance_report = self.dashboard.generate_performance_report(start_time, end_time)
            enhanced_outputs["dashboard_performance_report"] = performance_report
            
            # Export metrics data
            metrics_csv = self.dashboard.export_historical_data(
                "avg_fps", start_time, end_time, "csv"
            )
            enhanced_outputs["metrics_export"] = {
                "csv_data": metrics_csv[:1000] + "..." if len(metrics_csv) > 1000 else metrics_csv,
                "total_length": len(metrics_csv)
            }
        
        return enhanced_outputs
    
    def _display_enhanced_results(self, enhanced_outputs: dict):
        """Display comprehensive enhanced results"""
        
        print(f"\n{'='*100}")
        print("ğŸ¯ ENHANCED LOAD TEST RESULTS SUMMARY")
        print(f"{'='*100}")
        
        # Basic test info
        test_info = enhanced_outputs.get("test_info", {})
        print(f"\nğŸ“‹ Test Configuration:")
        print(f"   Target streams: {test_info.get('stream_count')}")
        print(f"   Test duration: {test_info.get('test_duration')}s")
        print(f"   Actual duration: {test_info.get('actual_duration', 0):.1f}s")
        print(f"   Enhanced features: {'âœ… Enabled' if test_info.get('enhanced_features_enabled') else 'âŒ Disabled'}")
        
        # Format results summary
        format_results = enhanced_outputs.get("format_results", {})
        print(f"\nğŸ“„ Generated Outputs:")
        for audience, files in format_results.items():
            if isinstance(files, (str, dict)):
                print(f"   {audience}: Generated")
        
        # Navigation files
        navigation_files = enhanced_outputs.get("navigation_files", {})
        print(f"\nğŸ—‚ï¸  Navigation Interfaces:")
        for audience, file_path in navigation_files.items():
            print(f"   {audience.title()}: {file_path}")
        
        # Actionable insights summary
        insights = enhanced_outputs.get("actionable_insights", {})
        if insights:
            summary = insights.get("summary", {})
            print(f"\nğŸ’¡ Actionable Insights:")
            print(f"   Overall health: {summary.get('overall_health', 'Unknown')}")
            print(f"   Total recommendations: {summary.get('total_recommendations', 0)}")
            print(f"   Critical issues: {summary.get('critical_issues', 0)}")
            print(f"   High priority issues: {summary.get('high_priority_issues', 0)}")
            print(f"   Estimated effort: {summary.get('estimated_total_effort', 'Unknown')}")
            
            # Top 3 actions
            top_actions = summary.get("top_3_actions", [])
            if top_actions:
                print(f"\nğŸ”¥ Top Priority Actions:")
                for i, action in enumerate(top_actions, 1):
                    print(f"   {i}. {action.get('title')} (Priority: {action.get('priority')}, Effort: {action.get('effort')})")
        
        # Integration results
        integration_results = enhanced_outputs.get("integration_results", {})
        if integration_results:
            print(f"\nğŸ”— Integration Results:")
            print(f"   Integrations attempted: {integration_results.get('integrations_attempted', 0)}")
            print(f"   Successful: {integration_results.get('integrations_successful', 0)}")
            print(f"   Failed: {integration_results.get('integrations_failed', 0)}")
        
        # Dashboard info
        if self.enable_dashboard:
            dashboard_report = enhanced_outputs.get("dashboard_performance_report", {})
            if dashboard_report:
                print(f"\nğŸ“Š Dashboard Analysis:")
                print(f"   Data points collected: {dashboard_report.get('data_points', 0)}")
                print(f"   Anomalies detected: {dashboard_report.get('anomalies_detected', 0)}")
                print(f"   Alerts triggered: {dashboard_report.get('alerts_triggered', 0)}")
                print(f"   Dashboard URL: http://localhost:5000")
        
        # File locations
        print(f"\nğŸ“ Output Locations:")
        print(f"   Enhanced reports: ./enhanced_reports/")
        print(f"   Organized reports: ./organized_reports/") 
        print(f"   Navigation UIs: ./organized_reports/")
        print(f"   Base reports: ./reports/")
        if self.enable_dashboard:
            print(f"   Metrics database: ./load_test_metrics.db")
        
        # Usage recommendations
        print(f"\nğŸ’¼ Usage Recommendations:")
        print(f"   ğŸ“Š Executives: Open navigation UI for executive dashboard")
        print(f"   ğŸ”§ Engineers: Review technical analysis and recommendations")
        print(f"   ğŸ“ˆ Data Scientists: Analyze detailed JSON reports and CSV exports")
        print(f"   ğŸš¨ Operations: Monitor real-time dashboard and alerts")
        
        print(f"\n{'='*100}")
        
        # Performance summary from base report
        base_report = enhanced_outputs.get("base_report", {})
        if "analysis" in base_report:
            analysis = base_report["analysis"]
            print(f"\nğŸ“ˆ Performance Summary:")
            print(f"   {analysis.get('summary', 'No summary available')}")
            
            perf_char = analysis.get("performance_characteristics", {})
            if perf_char:
                print(f"\nğŸ“Š Key Metrics:")
                print(f"   Achievement rate: {perf_char.get('achievement_rate', 0)}%")
                print(f"   Average FPS: {perf_char.get('avg_fps_per_stream', 0):.1f}")
                print(f"   Total data processed: {perf_char.get('total_data_gb', 0):.2f} GB")
                print(f"   Reconnection rate: {perf_char.get('reconnection_rate', 0):.3f}")
        
        print(f"\n{'='*100}")

def print_enhanced_usage():
    """Print usage information for enhanced test"""
    print("Enhanced Direct Camera Stream Load Testing Tool")
    print("=" * 50)
    print("\nUsage:")
    print("  python enhanced_direct_stream_test.py <stream_count> [duration] [options]")
    print("\nArguments:")
    print("  stream_count    Number of concurrent streams to test")
    print("  duration        Test duration in seconds (default: 120)")
    print("\nOptions:")
    print("  --dashboard     Enable real-time dashboard (starts web server)")
    print("  --help         Show this help message")
    print("\nExamples:")
    print("  python enhanced_direct_stream_test.py 4")
    print("  python enhanced_direct_stream_test.py 4 300")
    print("  python enhanced_direct_stream_test.py 4 300 --dashboard")
    print("\nOutput Features:")
    print("  âœ… Multi-format reports (JSON, CSV, HTML, PDF)")
    print("  âœ… Hierarchical navigation with filtering")
    print("  âœ… Actionable insights and recommendations")
    print("  âœ… Integration with monitoring systems")
    print("  âœ… Real-time dashboard (with --dashboard)")
    print("  âœ… Mobile-responsive design")
    print("  âœ… Executive, engineer, data scientist, and operations views")

async def main():
    """Main entry point for enhanced direct load testing"""
    
    # Parse arguments
    if len(sys.argv) < 2 or "--help" in sys.argv:
        print_enhanced_usage()
        sys.exit(0 if "--help" in sys.argv else 1)
    
    try:
        stream_count = int(sys.argv[1])
        duration = int(sys.argv[2]) if len(sys.argv) > 2 and sys.argv[2] != "--dashboard" else 120
        enable_dashboard = "--dashboard" in sys.argv
    except ValueError:
        print("âŒ Error: Please provide valid numbers for stream count and duration")
        sys.exit(1)
    
    if stream_count <= 0:
        print("âŒ Error: Stream count must be > 0")
        sys.exit(1)
    
    # Display startup information
    print("=" * 100)
    print("ğŸš€ ENHANCED CAMERA STREAM LOAD TESTING SYSTEM")
    print("=" * 100)
    print()
    print(f"ğŸ”§ Configuration:")
    print(f"   Target streams: {stream_count}")
    print(f"   Test duration: {duration}s")
    print(f"   Enhanced outputs: âœ… Enabled")
    print(f"   Real-time dashboard: {'âœ… Enabled' if enable_dashboard else 'âŒ Disabled'}")
    print(f"   Camera selection: Shuffled for variety")
    print()
    
    if enable_dashboard:
        print("ğŸ“Š Dashboard will be available at: http://localhost:5000")
        print("   - Executive Dashboard: /dashboard/executive") 
        print("   - Technical Dashboard: /dashboard/technical")
        print("   - Operations Dashboard: /dashboard/operations")
        print()
    
    print("ğŸ“ Output will be generated in:")
    print("   - ./enhanced_reports/ (multi-format outputs)")
    print("   - ./organized_reports/ (hierarchical navigation)")
    print("   - ./reports/ (base reports)")
    print()
    
    input("Press Enter to start enhanced testing...")
    
    # Create and run enhanced tester
    tester = EnhancedDirectLoadTester(
        stream_count=stream_count,
        test_duration=duration,
        enable_dashboard=enable_dashboard
    )
    
    try:
        start_time = time.time()
        results = await tester.run_enhanced_test()
        total_time = time.time() - start_time
        
        if "error" in results:
            print(f"âŒ Enhanced test failed: {results['error']}")
            return 1
        
        print(f"\nâ±ï¸  Total enhanced testing time: {total_time/60:.1f} minutes")
        
        if enable_dashboard:
            print(f"\nğŸ“Š Dashboard remains available at http://localhost:5000")
            print("   Press Ctrl+C to stop the dashboard server")
            
            # Keep dashboard running
            try:
                while True:
                    await asyncio.sleep(1)
            except KeyboardInterrupt:
                print("\nğŸ‘‹ Stopping dashboard server...")
        
        return 0
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸  Enhanced test interrupted by user")
        return 130
    except Exception as e:
        print(f"âŒ Enhanced test failed: {e}")
        return 1

if __name__ == "__main__":
    # Set up asyncio event loop policy for compatibility
    if sys.platform.startswith('win'):
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    
    result = asyncio.run(main())
    sys.exit(result)